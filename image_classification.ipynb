{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CIFAR-10 Dataset: 171MB [01:59, 1.43MB/s]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saj1919/Desktop/Learning_RL/pyconda3/anaconda3/lib/python3.6/site-packages/matplotlib/font_manager.py:279: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  'Matplotlib is building the font cache using fc-list. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nN40TWAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1129cef28>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    ### Max Value is 255, hence using devide-by-255\n",
    "    ### MinMax Scalar is option if results are not good\n",
    "    x_norm = x/255\n",
    "    return x_norm\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "Look into LabelBinarizer in the preprocessing module of sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "lb.fit([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    \n",
    "    x_one_hot = lb.transform(x)\n",
    "    return x_one_hot\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, shape=(None, *image_shape), name='x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, shape=(None, n_classes), name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers.\n",
    "\n",
    "** Hint: **\n",
    "\n",
    "When unpacking values as an argument in Python, look into the [unpacking](https://docs.python.org/3/tutorial/controlflow.html#unpacking-argument-lists) operator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    #print(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    #print(x_tensor.get_shape().as_list())\n",
    "    \n",
    "    depth = x_tensor.get_shape().as_list()[3]\n",
    "    weight = tf.Variable(tf.truncated_normal([*conv_ksize, depth, conv_num_outputs], mean=0.0, stddev=0.01))\n",
    "    bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    padding = 'SAME'\n",
    "    \n",
    "    conv = tf.nn.conv2d(x_tensor, weight, padding=padding, strides=[1,*conv_strides,1], use_cudnn_on_gpu=False)\n",
    "    conv = tf.nn.bias_add(conv, bias)\n",
    "    conv = tf.nn.relu(conv)\n",
    "    \n",
    "    return tf.nn.max_pool(value=conv, ksize=[1,*pool_ksize,1], strides=[1,*pool_strides,1], padding=padding, name='max_pool')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    #print(x_tensor)\n",
    "    dim = None\n",
    "    for curr_dim in x_tensor.get_shape().as_list()[1:]:\n",
    "        if dim is None:\n",
    "            dim = curr_dim\n",
    "        else:\n",
    "            dim *= curr_dim\n",
    "        #print(curr_dim, dim)\n",
    "    #print(tf.reshape(x_tensor, [-1, dim]))\n",
    "    return tf.reshape(x_tensor, [-1, dim])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    #print(x_tensor)\n",
    "    batch_size = x_tensor.get_shape().as_list()\n",
    "    batch_size = batch_size[-1]\n",
    "    #print(batch_size)\n",
    "    weights = tf.Variable(tf.truncated_normal([batch_size, num_outputs], mean=0.0, stddev=0.01))\n",
    "    bias = tf.Variable(tf.zeros([num_outputs]))\n",
    "    linear = tf.nn.bias_add(tf.matmul(x_tensor, weights), bias)\n",
    "    return tf.nn.relu(linear)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder_16:0\", shape=(?, 128), dtype=float32) 40\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    #print(x_tensor, num_outputs)\n",
    "    depth = x_tensor.get_shape().as_list()[-1]\n",
    "    weights = tf.Variable(tf.random_normal([depth, num_outputs], mean=0.0, stddev=0.01))\n",
    "    bias = tf.Variable(tf.zeros([num_outputs]))\n",
    "    return tf.nn.bias_add(tf.matmul(x_tensor, weights), bias)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"dropout_4/mul:0\", shape=(?, 128), dtype=float32) 10\n",
      "Tensor(\"dropout_9/mul:0\", shape=(?, 128), dtype=float32) 10\n",
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    conv_num_outputs = [64, 128, 256]\n",
    "    conv_strides = [(2,2), (3,3), (4,4)]\n",
    "    conv_ksize = [(4,4), (6,6), (8,8)]\n",
    "    pool_ksize = (4,4)\n",
    "    pool_strides = (2,2)\n",
    "    \n",
    "    conv_layer_1 = conv2d_maxpool(x, conv_num_outputs[0], conv_ksize[0], conv_strides[0], pool_ksize, pool_strides)\n",
    "    conv_layer_1 = tf.nn.dropout(conv_layer_1, keep_prob)\n",
    "    conv_layer_2 = conv2d_maxpool(conv_layer_1, conv_num_outputs[1], conv_ksize[1], conv_strides[1], pool_ksize, pool_strides)\n",
    "    conv_layer_2 = tf.nn.dropout(conv_layer_2, keep_prob)\n",
    "    conv_layer_3 = conv2d_maxpool(conv_layer_2, conv_num_outputs[2], conv_ksize[2], conv_strides[2], pool_ksize, pool_strides)\n",
    "    conv_layer_3 = tf.nn.dropout(conv_layer_3, keep_prob)\n",
    "    \n",
    "    x_tensor = flatten(conv_layer_3)\n",
    "    \n",
    "    num_outputs = (64, 128)\n",
    "    fully_conn_1 = fully_conn(x_tensor, num_outputs[0])\n",
    "    fully_conn_1 = tf.nn.dropout(fully_conn_1, keep_prob)\n",
    "    fully_conn_2 = fully_conn(fully_conn_1, num_outputs[1])\n",
    "    fully_conn_2 = tf.nn.dropout(fully_conn_2, keep_prob)\n",
    "    final_out = output(fully_conn_2, 10)\n",
    "\n",
    "    return final_out\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    session.run(optimizer, feed_dict={x: feature_batch, \n",
    "                                      y: label_batch, \n",
    "                                      keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    loss = sess.run(cost, feed_dict={x: feature_batch, y: label_batch, keep_prob: 1.})\n",
    "    accuracy = sess.run(accuracy, feed_dict={x: valid_features, y: valid_labels, keep_prob: 1.})\n",
    "    print('Loss - ', loss,\"Accuracy - \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 128\n",
    "batch_size = 128\n",
    "keep_probability = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss -  2.30323 Accuracy -  0.0998\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss -  2.23005 Accuracy -  0.1794\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss -  2.10539 Accuracy -  0.1986\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss -  2.05064 Accuracy -  0.208\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss -  2.0424 Accuracy -  0.204\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss -  1.9588 Accuracy -  0.22\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss -  1.93149 Accuracy -  0.2324\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss -  1.82847 Accuracy -  0.3044\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss -  1.76717 Accuracy -  0.3294\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss -  1.64243 Accuracy -  0.3402\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss -  1.63826 Accuracy -  0.3522\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss -  1.55343 Accuracy -  0.3544\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss -  1.5131 Accuracy -  0.3624\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss -  1.46278 Accuracy -  0.3792\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss -  1.4453 Accuracy -  0.3696\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss -  1.37259 Accuracy -  0.3888\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss -  1.34284 Accuracy -  0.4006\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss -  1.26956 Accuracy -  0.4034\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss -  1.24242 Accuracy -  0.4294\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss -  1.16681 Accuracy -  0.4366\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss -  1.12019 Accuracy -  0.4532\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss -  1.06128 Accuracy -  0.4574\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss -  1.06073 Accuracy -  0.4656\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss -  0.975852 Accuracy -  0.479\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss -  0.993829 Accuracy -  0.4708\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss -  0.946398 Accuracy -  0.484\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss -  0.915226 Accuracy -  0.478\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss -  0.89718 Accuracy -  0.498\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss -  0.810078 Accuracy -  0.508\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss -  0.829978 Accuracy -  0.5012\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss -  0.823576 Accuracy -  0.5018\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss -  0.764308 Accuracy -  0.5078\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss -  0.779241 Accuracy -  0.5114\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss -  0.749093 Accuracy -  0.5156\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss -  0.738652 Accuracy -  0.5226\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss -  0.630264 Accuracy -  0.5242\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss -  0.598556 Accuracy -  0.5294\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss -  0.645137 Accuracy -  0.518\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss -  0.620684 Accuracy -  0.5248\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss -  0.564884 Accuracy -  0.5286\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss -  0.516931 Accuracy -  0.533\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss -  0.524124 Accuracy -  0.513\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss -  0.549727 Accuracy -  0.5356\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss -  0.548665 Accuracy -  0.5242\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss -  0.468756 Accuracy -  0.5328\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss -  0.490137 Accuracy -  0.5288\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss -  0.486085 Accuracy -  0.5412\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss -  0.450636 Accuracy -  0.5362\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss -  0.460271 Accuracy -  0.537\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss -  0.452099 Accuracy -  0.536\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss -  0.469637 Accuracy -  0.5428\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss -  0.455273 Accuracy -  0.5392\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss -  0.449522 Accuracy -  0.547\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss -  0.416973 Accuracy -  0.5378\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss -  0.440466 Accuracy -  0.5378\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss -  0.404628 Accuracy -  0.5402\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss -  0.399501 Accuracy -  0.546\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss -  0.406195 Accuracy -  0.5314\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss -  0.426603 Accuracy -  0.5474\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss -  0.38951 Accuracy -  0.5474\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss -  0.373962 Accuracy -  0.548\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss -  0.366587 Accuracy -  0.556\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss -  0.33921 Accuracy -  0.5524\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss -  0.339699 Accuracy -  0.5562\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss -  0.331508 Accuracy -  0.5432\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss -  0.316766 Accuracy -  0.5542\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss -  0.325523 Accuracy -  0.5568\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss -  0.327992 Accuracy -  0.553\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss -  0.317078 Accuracy -  0.5558\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss -  0.324136 Accuracy -  0.554\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss -  0.308342 Accuracy -  0.556\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss -  0.342691 Accuracy -  0.548\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss -  0.309066 Accuracy -  0.5574\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss -  0.306155 Accuracy -  0.5466\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss -  0.324375 Accuracy -  0.542\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss -  0.288797 Accuracy -  0.5618\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss -  0.252544 Accuracy -  0.5616\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss -  0.288555 Accuracy -  0.5536\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss -  0.275319 Accuracy -  0.5586\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss -  0.269025 Accuracy -  0.5582\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss -  0.236798 Accuracy -  0.5558\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss -  0.219753 Accuracy -  0.5552\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss -  0.262354 Accuracy -  0.5544\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss -  0.237045 Accuracy -  0.5556\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss -  0.189616 Accuracy -  0.551\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss -  0.192613 Accuracy -  0.5552\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss -  0.222146 Accuracy -  0.5524\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss -  0.204349 Accuracy -  0.5536\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss -  0.198479 Accuracy -  0.5568\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss -  0.192716 Accuracy -  0.5598\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss -  0.181388 Accuracy -  0.5584\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss -  0.179569 Accuracy -  0.5608\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss -  0.220361 Accuracy -  0.5532\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss -  0.192653 Accuracy -  0.558\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss -  0.172951 Accuracy -  0.5596\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss -  0.198584 Accuracy -  0.5674\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss -  0.201437 Accuracy -  0.5646\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss -  0.18748 Accuracy -  0.5572\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss -  0.165921 Accuracy -  0.5652\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss -  0.161208 Accuracy -  0.5616\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss -  0.176227 Accuracy -  0.5538\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss -  0.175359 Accuracy -  0.5678\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss -  0.166282 Accuracy -  0.559\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss -  0.182346 Accuracy -  0.5618\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss -  0.154044 Accuracy -  0.5584\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss -  0.178818 Accuracy -  0.5642\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss -  0.179463 Accuracy -  0.5562\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss -  0.169885 Accuracy -  0.5692\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss -  0.163987 Accuracy -  0.5648\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss -  0.178255 Accuracy -  0.5604\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss -  0.153263 Accuracy -  0.5642\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss -  0.127684 Accuracy -  0.5626\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss -  0.161841 Accuracy -  0.5576\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss -  0.144291 Accuracy -  0.5552\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss -  0.144832 Accuracy -  0.562\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss -  0.123793 Accuracy -  0.5662\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss -  0.122773 Accuracy -  0.562\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss -  0.145132 Accuracy -  0.5588\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss -  0.126251 Accuracy -  0.551\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss -  0.133679 Accuracy -  0.554\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss -  0.120662 Accuracy -  0.5466\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss -  0.101372 Accuracy -  0.559\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss -  0.102976 Accuracy -  0.557\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss -  0.103322 Accuracy -  0.5578\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss -  0.0922451 Accuracy -  0.5658\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss -  0.0907872 Accuracy -  0.5608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 127, CIFAR-10 Batch 1:  Loss -  0.0883612 Accuracy -  0.5518\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss -  0.0996353 Accuracy -  0.5534\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss -  2.28806 Accuracy -  0.105\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss -  1.95328 Accuracy -  0.1964\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss -  1.76825 Accuracy -  0.1812\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss -  1.91681 Accuracy -  0.2216\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss -  1.8965 Accuracy -  0.2138\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss -  1.99097 Accuracy -  0.2444\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss -  1.92731 Accuracy -  0.2754\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss -  1.52262 Accuracy -  0.3046\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss -  1.76302 Accuracy -  0.319\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss -  1.63949 Accuracy -  0.329\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss -  1.75752 Accuracy -  0.3484\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss -  1.73295 Accuracy -  0.3556\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss -  1.25863 Accuracy -  0.377\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss -  1.57894 Accuracy -  0.3836\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss -  1.54151 Accuracy -  0.389\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss -  1.61032 Accuracy -  0.395\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss -  1.66088 Accuracy -  0.4136\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss -  1.15016 Accuracy -  0.4246\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss -  1.56274 Accuracy -  0.4244\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss -  1.43176 Accuracy -  0.4172\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss -  1.4353 Accuracy -  0.4534\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss -  1.46533 Accuracy -  0.457\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss -  1.14501 Accuracy -  0.4654\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss -  1.39101 Accuracy -  0.4728\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss -  1.39495 Accuracy -  0.4636\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss -  1.30061 Accuracy -  0.4972\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss -  1.37194 Accuracy -  0.5022\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss -  1.12488 Accuracy -  0.5034\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss -  1.2928 Accuracy -  0.511\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss -  1.31665 Accuracy -  0.4932\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss -  1.30476 Accuracy -  0.5044\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss -  1.23503 Accuracy -  0.515\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss -  1.11234 Accuracy -  0.4996\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss -  1.22164 Accuracy -  0.5396\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss -  1.25822 Accuracy -  0.52\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss -  1.22055 Accuracy -  0.5468\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss -  1.18987 Accuracy -  0.5432\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss -  1.00205 Accuracy -  0.5402\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss -  1.18032 Accuracy -  0.556\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss -  1.16587 Accuracy -  0.5522\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss -  1.15071 Accuracy -  0.5544\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss -  1.09454 Accuracy -  0.5676\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss -  0.958263 Accuracy -  0.5538\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss -  1.13157 Accuracy -  0.5618\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss -  1.11186 Accuracy -  0.5682\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss -  1.07793 Accuracy -  0.5694\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss -  0.993035 Accuracy -  0.5698\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss -  0.918571 Accuracy -  0.5646\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss -  1.0316 Accuracy -  0.5862\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss -  1.03981 Accuracy -  0.5738\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss -  0.9874 Accuracy -  0.5776\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss -  0.973622 Accuracy -  0.5822\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss -  0.818128 Accuracy -  0.575\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss -  1.01259 Accuracy -  0.5874\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss -  1.05524 Accuracy -  0.59\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss -  0.921969 Accuracy -  0.5964\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss -  0.938941 Accuracy -  0.5964\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss -  0.7484 Accuracy -  0.5982\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss -  0.935702 Accuracy -  0.5982\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss -  0.987928 Accuracy -  0.5916\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss -  0.828792 Accuracy -  0.5986\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss -  0.874936 Accuracy -  0.6048\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss -  0.736339 Accuracy -  0.5956\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss -  0.989362 Accuracy -  0.6018\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss -  0.899002 Accuracy -  0.5984\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss -  0.87573 Accuracy -  0.6026\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss -  0.908906 Accuracy -  0.6062\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss -  0.668172 Accuracy -  0.5864\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss -  0.876782 Accuracy -  0.6148\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss -  0.809258 Accuracy -  0.6144\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss -  0.824467 Accuracy -  0.6208\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss -  0.8215 Accuracy -  0.6128\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss -  0.646519 Accuracy -  0.5994\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss -  0.900683 Accuracy -  0.6148\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss -  0.808499 Accuracy -  0.6106\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss -  0.768946 Accuracy -  0.6212\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss -  0.817176 Accuracy -  0.622\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss -  0.630838 Accuracy -  0.6146\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss -  0.787497 Accuracy -  0.6208\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss -  0.749093 Accuracy -  0.6158\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss -  0.729361 Accuracy -  0.625\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss -  0.785021 Accuracy -  0.6256\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss -  0.610138 Accuracy -  0.629\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss -  0.831188 Accuracy -  0.6302\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss -  0.807716 Accuracy -  0.6298\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss -  0.753786 Accuracy -  0.6284\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss -  0.776413 Accuracy -  0.6362\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss -  0.597472 Accuracy -  0.6264\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss -  0.761511 Accuracy -  0.6426\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss -  0.713988 Accuracy -  0.63\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss -  0.704846 Accuracy -  0.6266\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss -  0.739826 Accuracy -  0.6308\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss -  0.607287 Accuracy -  0.624\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss -  0.804114 Accuracy -  0.6308\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss -  0.709648 Accuracy -  0.636\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss -  0.61405 Accuracy -  0.6266\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss -  0.748272 Accuracy -  0.6404\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss -  0.488828 Accuracy -  0.6392\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss -  0.757313 Accuracy -  0.6366\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss -  0.711663 Accuracy -  0.6382\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss -  0.671561 Accuracy -  0.6436\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss -  0.683525 Accuracy -  0.636\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss -  0.506992 Accuracy -  0.6294\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss -  0.793609 Accuracy -  0.6266\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss -  0.645682 Accuracy -  0.6462\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss -  0.649139 Accuracy -  0.6438\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss -  0.715166 Accuracy -  0.629\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss -  0.588874 Accuracy -  0.6496\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss -  0.700501 Accuracy -  0.6406\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss -  0.552787 Accuracy -  0.645\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss -  0.642657 Accuracy -  0.6392\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss -  0.649387 Accuracy -  0.6518\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss -  0.519017 Accuracy -  0.6504\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss -  0.724322 Accuracy -  0.6388\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss -  0.60116 Accuracy -  0.6444\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss -  0.620638 Accuracy -  0.6516\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss -  0.617832 Accuracy -  0.6572\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss -  0.485204 Accuracy -  0.6502\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss -  0.667474 Accuracy -  0.6496\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss -  0.566718 Accuracy -  0.6438\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss -  0.605727 Accuracy -  0.652\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss -  0.660219 Accuracy -  0.6506\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss -  0.453648 Accuracy -  0.6578\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss -  0.665187 Accuracy -  0.6566\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss -  0.570026 Accuracy -  0.6588\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss -  0.57467 Accuracy -  0.6576\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss -  0.605448 Accuracy -  0.6534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, CIFAR-10 Batch 3:  Loss -  0.441217 Accuracy -  0.6612\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss -  0.653099 Accuracy -  0.6534\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss -  0.518319 Accuracy -  0.6572\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss -  0.573166 Accuracy -  0.6564\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss -  0.608337 Accuracy -  0.655\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss -  0.448541 Accuracy -  0.6534\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss -  0.576451 Accuracy -  0.6624\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss -  0.454366 Accuracy -  0.6604\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss -  0.596581 Accuracy -  0.664\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss -  0.549478 Accuracy -  0.663\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss -  0.425765 Accuracy -  0.66\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss -  0.627122 Accuracy -  0.6514\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss -  0.450753 Accuracy -  0.6634\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss -  0.583098 Accuracy -  0.6662\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss -  0.560209 Accuracy -  0.6586\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss -  0.347489 Accuracy -  0.6602\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss -  0.618394 Accuracy -  0.651\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss -  0.472727 Accuracy -  0.6652\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss -  0.585701 Accuracy -  0.66\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss -  0.563763 Accuracy -  0.6632\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss -  0.410648 Accuracy -  0.666\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss -  0.536 Accuracy -  0.666\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss -  0.490802 Accuracy -  0.6746\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss -  0.638687 Accuracy -  0.6674\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss -  0.5202 Accuracy -  0.6588\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss -  0.400933 Accuracy -  0.6712\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss -  0.582899 Accuracy -  0.6628\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss -  0.435525 Accuracy -  0.67\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss -  0.568506 Accuracy -  0.6672\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss -  0.608853 Accuracy -  0.667\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss -  0.415737 Accuracy -  0.669\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss -  0.552515 Accuracy -  0.658\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss -  0.472205 Accuracy -  0.6714\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss -  0.5941 Accuracy -  0.666\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss -  0.513102 Accuracy -  0.6648\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss -  0.418718 Accuracy -  0.6648\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss -  0.52577 Accuracy -  0.6708\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss -  0.441501 Accuracy -  0.6716\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss -  0.476568 Accuracy -  0.665\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss -  0.542287 Accuracy -  0.6686\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss -  0.35216 Accuracy -  0.6726\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss -  0.489232 Accuracy -  0.6732\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss -  0.4071 Accuracy -  0.6726\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss -  0.534296 Accuracy -  0.67\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss -  0.510435 Accuracy -  0.6702\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss -  0.363215 Accuracy -  0.6672\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss -  0.485449 Accuracy -  0.665\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss -  0.437156 Accuracy -  0.6748\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss -  0.49682 Accuracy -  0.6592\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss -  0.511102 Accuracy -  0.67\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss -  0.331789 Accuracy -  0.6732\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss -  0.458463 Accuracy -  0.6694\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss -  0.434333 Accuracy -  0.6696\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss -  0.559255 Accuracy -  0.6664\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss -  0.574733 Accuracy -  0.6672\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss -  0.369315 Accuracy -  0.6702\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss -  0.506391 Accuracy -  0.6656\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss -  0.463686 Accuracy -  0.6646\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss -  0.453272 Accuracy -  0.6764\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss -  0.534591 Accuracy -  0.6714\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss -  0.342664 Accuracy -  0.6656\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss -  0.412043 Accuracy -  0.6746\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss -  0.422749 Accuracy -  0.6686\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss -  0.479817 Accuracy -  0.666\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss -  0.464489 Accuracy -  0.6672\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss -  0.349855 Accuracy -  0.6704\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss -  0.486184 Accuracy -  0.6704\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss -  0.424042 Accuracy -  0.6506\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss -  0.485496 Accuracy -  0.6794\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss -  0.431994 Accuracy -  0.676\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss -  0.319979 Accuracy -  0.6742\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss -  0.429671 Accuracy -  0.6676\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss -  0.401012 Accuracy -  0.6676\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss -  0.511921 Accuracy -  0.6758\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss -  0.457485 Accuracy -  0.676\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss -  0.326051 Accuracy -  0.6672\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss -  0.432838 Accuracy -  0.6658\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss -  0.443231 Accuracy -  0.6588\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss -  0.479357 Accuracy -  0.6708\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss -  0.445583 Accuracy -  0.6748\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss -  0.368729 Accuracy -  0.6552\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss -  0.454583 Accuracy -  0.6684\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss -  0.391585 Accuracy -  0.6638\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss -  0.534676 Accuracy -  0.6712\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss -  0.45191 Accuracy -  0.6708\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss -  0.343692 Accuracy -  0.6682\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss -  0.442552 Accuracy -  0.667\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss -  0.392333 Accuracy -  0.6566\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss -  0.455489 Accuracy -  0.6762\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss -  0.460685 Accuracy -  0.666\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss -  0.310873 Accuracy -  0.6728\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss -  0.47395 Accuracy -  0.67\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss -  0.402674 Accuracy -  0.6674\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss -  0.423968 Accuracy -  0.678\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss -  0.467429 Accuracy -  0.673\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss -  0.320471 Accuracy -  0.6744\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss -  0.430061 Accuracy -  0.679\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss -  0.377023 Accuracy -  0.6714\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss -  0.458833 Accuracy -  0.673\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss -  0.463239 Accuracy -  0.6712\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss -  0.311962 Accuracy -  0.6732\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss -  0.421308 Accuracy -  0.6722\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss -  0.414147 Accuracy -  0.6686\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss -  0.46615 Accuracy -  0.671\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss -  0.408858 Accuracy -  0.671\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss -  0.330412 Accuracy -  0.6598\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss -  0.374617 Accuracy -  0.674\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss -  0.417057 Accuracy -  0.667\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss -  0.441864 Accuracy -  0.6794\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss -  0.403762 Accuracy -  0.6782\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss -  0.347166 Accuracy -  0.6754\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss -  0.3917 Accuracy -  0.6772\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss -  0.340682 Accuracy -  0.676\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss -  0.422593 Accuracy -  0.6768\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss -  0.42805 Accuracy -  0.676\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss -  0.335119 Accuracy -  0.6758\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss -  0.399878 Accuracy -  0.6788\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss -  0.363135 Accuracy -  0.6738\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss -  0.460492 Accuracy -  0.67\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss -  0.425291 Accuracy -  0.6752\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss -  0.274045 Accuracy -  0.6748\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss -  0.390138 Accuracy -  0.6736\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss -  0.316385 Accuracy -  0.666\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss -  0.481949 Accuracy -  0.6716\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss -  0.447151 Accuracy -  0.6744\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss -  0.297199 Accuracy -  0.6754\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss -  0.430819 Accuracy -  0.6746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51, CIFAR-10 Batch 5:  Loss -  0.322717 Accuracy -  0.667\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss -  0.390635 Accuracy -  0.6706\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss -  0.426317 Accuracy -  0.679\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss -  0.300031 Accuracy -  0.6734\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss -  0.419814 Accuracy -  0.676\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss -  0.304902 Accuracy -  0.6758\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss -  0.413615 Accuracy -  0.6782\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss -  0.394435 Accuracy -  0.6724\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss -  0.274197 Accuracy -  0.6806\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss -  0.369501 Accuracy -  0.6774\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss -  0.338673 Accuracy -  0.6814\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss -  0.449428 Accuracy -  0.6732\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss -  0.431106 Accuracy -  0.6784\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss -  0.28611 Accuracy -  0.677\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss -  0.3443 Accuracy -  0.6726\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss -  0.316595 Accuracy -  0.6752\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss -  0.387209 Accuracy -  0.6812\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss -  0.409916 Accuracy -  0.6734\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss -  0.310696 Accuracy -  0.6758\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss -  0.349826 Accuracy -  0.683\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss -  0.317603 Accuracy -  0.6732\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss -  0.387131 Accuracy -  0.6812\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss -  0.407386 Accuracy -  0.6786\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss -  0.319244 Accuracy -  0.6744\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss -  0.351781 Accuracy -  0.6786\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss -  0.332669 Accuracy -  0.6754\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss -  0.42882 Accuracy -  0.6818\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss -  0.359978 Accuracy -  0.676\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss -  0.328343 Accuracy -  0.6846\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss -  0.361148 Accuracy -  0.6724\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss -  0.292151 Accuracy -  0.671\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss -  0.361281 Accuracy -  0.68\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss -  0.359284 Accuracy -  0.682\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss -  0.296801 Accuracy -  0.6782\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss -  0.365098 Accuracy -  0.6794\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss -  0.288276 Accuracy -  0.6638\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss -  0.395943 Accuracy -  0.6826\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss -  0.363449 Accuracy -  0.681\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss -  0.304244 Accuracy -  0.679\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss -  0.349136 Accuracy -  0.6786\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss -  0.28713 Accuracy -  0.6732\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss -  0.400433 Accuracy -  0.689\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss -  0.370119 Accuracy -  0.6794\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss -  0.303431 Accuracy -  0.6782\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss -  0.357567 Accuracy -  0.6776\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss -  0.282586 Accuracy -  0.674\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss -  0.415189 Accuracy -  0.683\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss -  0.339326 Accuracy -  0.6872\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss -  0.273425 Accuracy -  0.686\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss -  0.376276 Accuracy -  0.6752\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss -  0.314608 Accuracy -  0.6796\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss -  0.45477 Accuracy -  0.6804\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss -  0.371066 Accuracy -  0.6796\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss -  0.268263 Accuracy -  0.6784\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss -  0.313672 Accuracy -  0.6796\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss -  0.293509 Accuracy -  0.6838\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss -  0.36557 Accuracy -  0.6792\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss -  0.353835 Accuracy -  0.6824\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss -  0.264101 Accuracy -  0.6808\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss -  0.323287 Accuracy -  0.6764\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss -  0.279565 Accuracy -  0.6786\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss -  0.31498 Accuracy -  0.6808\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss -  0.384091 Accuracy -  0.6762\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss -  0.241354 Accuracy -  0.681\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss -  0.346723 Accuracy -  0.6718\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss -  0.266719 Accuracy -  0.6684\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss -  0.353544 Accuracy -  0.6854\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss -  0.327994 Accuracy -  0.6906\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss -  0.225838 Accuracy -  0.6826\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss -  0.295062 Accuracy -  0.6834\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss -  0.27485 Accuracy -  0.6832\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss -  0.363056 Accuracy -  0.6896\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss -  0.343903 Accuracy -  0.682\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss -  0.24446 Accuracy -  0.6822\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss -  0.272506 Accuracy -  0.6844\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss -  0.281646 Accuracy -  0.6858\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss -  0.356177 Accuracy -  0.6786\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss -  0.309154 Accuracy -  0.6886\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss -  0.218618 Accuracy -  0.6836\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss -  0.324823 Accuracy -  0.6902\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss -  0.276889 Accuracy -  0.6806\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss -  0.364093 Accuracy -  0.6836\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss -  0.327731 Accuracy -  0.6866\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss -  0.260776 Accuracy -  0.673\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss -  0.337783 Accuracy -  0.6884\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss -  0.267407 Accuracy -  0.6808\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss -  0.342516 Accuracy -  0.6848\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss -  0.338338 Accuracy -  0.6792\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss -  0.251266 Accuracy -  0.6734\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss -  0.290506 Accuracy -  0.686\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss -  0.262566 Accuracy -  0.6828\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss -  0.341743 Accuracy -  0.6858\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss -  0.31914 Accuracy -  0.687\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss -  0.256985 Accuracy -  0.681\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss -  0.282049 Accuracy -  0.6812\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss -  0.339244 Accuracy -  0.6912\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss -  0.349276 Accuracy -  0.6874\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss -  0.346011 Accuracy -  0.6906\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss -  0.291824 Accuracy -  0.6754\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss -  0.327284 Accuracy -  0.6808\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss -  0.298938 Accuracy -  0.6856\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss -  0.391272 Accuracy -  0.6784\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss -  0.343 Accuracy -  0.682\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss -  0.245413 Accuracy -  0.6888\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss -  0.257539 Accuracy -  0.6784\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss -  0.277834 Accuracy -  0.6788\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss -  0.373112 Accuracy -  0.6858\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss -  0.371753 Accuracy -  0.6812\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss -  0.246771 Accuracy -  0.6822\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss -  0.283728 Accuracy -  0.6782\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss -  0.258272 Accuracy -  0.6764\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss -  0.317413 Accuracy -  0.6804\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss -  0.385004 Accuracy -  0.6778\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss -  0.205076 Accuracy -  0.6858\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss -  0.276115 Accuracy -  0.6768\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss -  0.286215 Accuracy -  0.6814\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss -  0.336303 Accuracy -  0.6806\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss -  0.384623 Accuracy -  0.6818\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss -  0.223076 Accuracy -  0.6814\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss -  0.277527 Accuracy -  0.6886\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss -  0.300525 Accuracy -  0.6886\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss -  0.31948 Accuracy -  0.6836\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss -  0.347334 Accuracy -  0.6876\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss -  0.213392 Accuracy -  0.6808\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss -  0.27237 Accuracy -  0.6876\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss -  0.238461 Accuracy -  0.6884\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss -  0.326728 Accuracy -  0.679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77, CIFAR-10 Batch 2:  Loss -  0.363364 Accuracy -  0.6904\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss -  0.216814 Accuracy -  0.6894\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss -  0.300294 Accuracy -  0.6812\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss -  0.293694 Accuracy -  0.6786\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss -  0.368345 Accuracy -  0.684\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss -  0.345283 Accuracy -  0.6856\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss -  0.220155 Accuracy -  0.6854\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss -  0.331483 Accuracy -  0.6798\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss -  0.274092 Accuracy -  0.679\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss -  0.338082 Accuracy -  0.6884\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss -  0.329182 Accuracy -  0.6814\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss -  0.207602 Accuracy -  0.681\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss -  0.307844 Accuracy -  0.6788\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss -  0.264389 Accuracy -  0.687\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss -  0.289649 Accuracy -  0.6846\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss -  0.291684 Accuracy -  0.69\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss -  0.186555 Accuracy -  0.687\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss -  0.31318 Accuracy -  0.6862\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss -  0.248451 Accuracy -  0.6828\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss -  0.350873 Accuracy -  0.6866\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss -  0.327901 Accuracy -  0.685\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss -  0.200327 Accuracy -  0.688\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss -  0.31407 Accuracy -  0.6784\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss -  0.259304 Accuracy -  0.6848\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss -  0.361914 Accuracy -  0.6714\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss -  0.326356 Accuracy -  0.6954\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss -  0.193925 Accuracy -  0.686\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss -  0.249931 Accuracy -  0.6856\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss -  0.258415 Accuracy -  0.6794\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss -  0.308062 Accuracy -  0.6902\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss -  0.311055 Accuracy -  0.6862\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss -  0.219349 Accuracy -  0.6828\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss -  0.305891 Accuracy -  0.6888\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss -  0.272567 Accuracy -  0.683\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss -  0.271923 Accuracy -  0.6924\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss -  0.315265 Accuracy -  0.6868\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss -  0.200758 Accuracy -  0.677\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss -  0.294536 Accuracy -  0.6848\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss -  0.275066 Accuracy -  0.682\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss -  0.320677 Accuracy -  0.6902\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss -  0.330706 Accuracy -  0.6842\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss -  0.204002 Accuracy -  0.6814\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss -  0.284055 Accuracy -  0.6806\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss -  0.235375 Accuracy -  0.688\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss -  0.253484 Accuracy -  0.6884\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss -  0.323085 Accuracy -  0.6882\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss -  0.213273 Accuracy -  0.6854\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss -  0.280898 Accuracy -  0.6838\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss -  0.21293 Accuracy -  0.6822\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss -  0.298407 Accuracy -  0.6726\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss -  0.301717 Accuracy -  0.6868\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss -  0.198084 Accuracy -  0.6862\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss -  0.257114 Accuracy -  0.6942\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss -  0.248551 Accuracy -  0.6858\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss -  0.297269 Accuracy -  0.6816\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss -  0.295921 Accuracy -  0.6836\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss -  0.195207 Accuracy -  0.6826\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss -  0.235332 Accuracy -  0.688\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss -  0.231789 Accuracy -  0.679\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss -  0.309184 Accuracy -  0.6878\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss -  0.305459 Accuracy -  0.6948\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss -  0.208883 Accuracy -  0.6864\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss -  0.259072 Accuracy -  0.6788\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss -  0.234598 Accuracy -  0.6798\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss -  0.305417 Accuracy -  0.6906\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss -  0.326502 Accuracy -  0.6856\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss -  0.205016 Accuracy -  0.6802\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss -  0.234796 Accuracy -  0.6944\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss -  0.234292 Accuracy -  0.6826\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss -  0.288485 Accuracy -  0.6828\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss -  0.320455 Accuracy -  0.688\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss -  0.184806 Accuracy -  0.6792\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss -  0.244008 Accuracy -  0.6908\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss -  0.225599 Accuracy -  0.6884\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss -  0.344288 Accuracy -  0.679\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss -  0.318237 Accuracy -  0.6944\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss -  0.193699 Accuracy -  0.6886\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss -  0.255789 Accuracy -  0.6834\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss -  0.214034 Accuracy -  0.6806\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss -  0.312523 Accuracy -  0.6886\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss -  0.329415 Accuracy -  0.697\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss -  0.221484 Accuracy -  0.6874\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss -  0.277072 Accuracy -  0.6902\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss -  0.222168 Accuracy -  0.6888\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss -  0.293902 Accuracy -  0.6806\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss -  0.339574 Accuracy -  0.6834\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss -  0.197944 Accuracy -  0.6908\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss -  0.250604 Accuracy -  0.6788\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss -  0.191171 Accuracy -  0.6794\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss -  0.323248 Accuracy -  0.6728\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss -  0.317372 Accuracy -  0.6834\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss -  0.185358 Accuracy -  0.6762\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss -  0.270318 Accuracy -  0.6826\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss -  0.238399 Accuracy -  0.6834\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss -  0.357356 Accuracy -  0.6832\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss -  0.305089 Accuracy -  0.6848\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss -  0.190988 Accuracy -  0.6794\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss -  0.254669 Accuracy -  0.6898\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss -  0.235247 Accuracy -  0.6786\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss -  0.347232 Accuracy -  0.674\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss -  0.310056 Accuracy -  0.691\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss -  0.192362 Accuracy -  0.6852\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss -  0.244011 Accuracy -  0.6842\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss -  0.244429 Accuracy -  0.6786\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss -  0.29954 Accuracy -  0.68\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss -  0.295607 Accuracy -  0.6854\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss -  0.193833 Accuracy -  0.6844\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss -  0.230529 Accuracy -  0.6862\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss -  0.222438 Accuracy -  0.6776\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss -  0.295065 Accuracy -  0.68\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss -  0.291869 Accuracy -  0.6822\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss -  0.231851 Accuracy -  0.6868\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss -  0.243604 Accuracy -  0.685\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss -  0.217021 Accuracy -  0.6862\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss -  0.30581 Accuracy -  0.6852\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss -  0.289715 Accuracy -  0.6928\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss -  0.186913 Accuracy -  0.6834\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss -  0.298012 Accuracy -  0.6898\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss -  0.265807 Accuracy -  0.6846\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss -  0.311246 Accuracy -  0.6786\n",
      "Epoch 101, CIFAR-10 Batch 2:  Loss -  0.278685 Accuracy -  0.6884\n",
      "Epoch 101, CIFAR-10 Batch 3:  Loss -  0.192533 Accuracy -  0.6854\n",
      "Epoch 101, CIFAR-10 Batch 4:  Loss -  0.242429 Accuracy -  0.6854\n",
      "Epoch 101, CIFAR-10 Batch 5:  Loss -  0.200536 Accuracy -  0.6822\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss -  0.281816 Accuracy -  0.68\n",
      "Epoch 102, CIFAR-10 Batch 2:  Loss -  0.271722 Accuracy -  0.6946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102, CIFAR-10 Batch 3:  Loss -  0.204697 Accuracy -  0.6898\n",
      "Epoch 102, CIFAR-10 Batch 4:  Loss -  0.254634 Accuracy -  0.686\n",
      "Epoch 102, CIFAR-10 Batch 5:  Loss -  0.205079 Accuracy -  0.6862\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss -  0.276264 Accuracy -  0.6834\n",
      "Epoch 103, CIFAR-10 Batch 2:  Loss -  0.260803 Accuracy -  0.6868\n",
      "Epoch 103, CIFAR-10 Batch 3:  Loss -  0.227336 Accuracy -  0.68\n",
      "Epoch 103, CIFAR-10 Batch 4:  Loss -  0.286632 Accuracy -  0.6826\n",
      "Epoch 103, CIFAR-10 Batch 5:  Loss -  0.186969 Accuracy -  0.6822\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss -  0.290225 Accuracy -  0.682\n",
      "Epoch 104, CIFAR-10 Batch 2:  Loss -  0.305279 Accuracy -  0.6852\n",
      "Epoch 104, CIFAR-10 Batch 3:  Loss -  0.229362 Accuracy -  0.6868\n",
      "Epoch 104, CIFAR-10 Batch 4:  Loss -  0.290335 Accuracy -  0.6816\n",
      "Epoch 104, CIFAR-10 Batch 5:  Loss -  0.237778 Accuracy -  0.6918\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss -  0.252411 Accuracy -  0.6866\n",
      "Epoch 105, CIFAR-10 Batch 2:  Loss -  0.296831 Accuracy -  0.6888\n",
      "Epoch 105, CIFAR-10 Batch 3:  Loss -  0.223972 Accuracy -  0.6898\n",
      "Epoch 105, CIFAR-10 Batch 4:  Loss -  0.281916 Accuracy -  0.681\n",
      "Epoch 105, CIFAR-10 Batch 5:  Loss -  0.230583 Accuracy -  0.6866\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss -  0.259638 Accuracy -  0.6822\n",
      "Epoch 106, CIFAR-10 Batch 2:  Loss -  0.28681 Accuracy -  0.6814\n",
      "Epoch 106, CIFAR-10 Batch 3:  Loss -  0.223996 Accuracy -  0.6794\n",
      "Epoch 106, CIFAR-10 Batch 4:  Loss -  0.258368 Accuracy -  0.6854\n",
      "Epoch 106, CIFAR-10 Batch 5:  Loss -  0.231285 Accuracy -  0.6842\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss -  0.284095 Accuracy -  0.6826\n",
      "Epoch 107, CIFAR-10 Batch 2:  Loss -  0.306453 Accuracy -  0.686\n",
      "Epoch 107, CIFAR-10 Batch 3:  Loss -  0.200914 Accuracy -  0.6884\n",
      "Epoch 107, CIFAR-10 Batch 4:  Loss -  0.272824 Accuracy -  0.6822\n",
      "Epoch 107, CIFAR-10 Batch 5:  Loss -  0.228331 Accuracy -  0.6872\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss -  0.308456 Accuracy -  0.6776\n",
      "Epoch 108, CIFAR-10 Batch 2:  Loss -  0.308267 Accuracy -  0.6916\n",
      "Epoch 108, CIFAR-10 Batch 3:  Loss -  0.204173 Accuracy -  0.6904\n",
      "Epoch 108, CIFAR-10 Batch 4:  Loss -  0.240986 Accuracy -  0.6872\n",
      "Epoch 108, CIFAR-10 Batch 5:  Loss -  0.195705 Accuracy -  0.6892\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss -  0.323837 Accuracy -  0.6844\n",
      "Epoch 109, CIFAR-10 Batch 2:  Loss -  0.309222 Accuracy -  0.6876\n",
      "Epoch 109, CIFAR-10 Batch 3:  Loss -  0.186789 Accuracy -  0.6828\n",
      "Epoch 109, CIFAR-10 Batch 4:  Loss -  0.239732 Accuracy -  0.6896\n",
      "Epoch 109, CIFAR-10 Batch 5:  Loss -  0.215812 Accuracy -  0.6848\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss -  0.249981 Accuracy -  0.6902\n",
      "Epoch 110, CIFAR-10 Batch 2:  Loss -  0.290126 Accuracy -  0.6952\n",
      "Epoch 110, CIFAR-10 Batch 3:  Loss -  0.186132 Accuracy -  0.678\n",
      "Epoch 110, CIFAR-10 Batch 4:  Loss -  0.21833 Accuracy -  0.6886\n",
      "Epoch 110, CIFAR-10 Batch 5:  Loss -  0.173581 Accuracy -  0.6892\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss -  0.288759 Accuracy -  0.6798\n",
      "Epoch 111, CIFAR-10 Batch 2:  Loss -  0.298105 Accuracy -  0.6866\n",
      "Epoch 111, CIFAR-10 Batch 3:  Loss -  0.206708 Accuracy -  0.6834\n",
      "Epoch 111, CIFAR-10 Batch 4:  Loss -  0.22635 Accuracy -  0.6846\n",
      "Epoch 111, CIFAR-10 Batch 5:  Loss -  0.183637 Accuracy -  0.6876\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss -  0.251992 Accuracy -  0.6858\n",
      "Epoch 112, CIFAR-10 Batch 2:  Loss -  0.307318 Accuracy -  0.6844\n",
      "Epoch 112, CIFAR-10 Batch 3:  Loss -  0.176603 Accuracy -  0.693\n",
      "Epoch 112, CIFAR-10 Batch 4:  Loss -  0.26614 Accuracy -  0.6904\n",
      "Epoch 112, CIFAR-10 Batch 5:  Loss -  0.204077 Accuracy -  0.6834\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss -  0.245085 Accuracy -  0.6844\n",
      "Epoch 113, CIFAR-10 Batch 2:  Loss -  0.321102 Accuracy -  0.692\n",
      "Epoch 113, CIFAR-10 Batch 3:  Loss -  0.179114 Accuracy -  0.6854\n",
      "Epoch 113, CIFAR-10 Batch 4:  Loss -  0.236013 Accuracy -  0.685\n",
      "Epoch 113, CIFAR-10 Batch 5:  Loss -  0.197464 Accuracy -  0.6818\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss -  0.281146 Accuracy -  0.6876\n",
      "Epoch 114, CIFAR-10 Batch 2:  Loss -  0.292613 Accuracy -  0.6884\n",
      "Epoch 114, CIFAR-10 Batch 3:  Loss -  0.192516 Accuracy -  0.6804\n",
      "Epoch 114, CIFAR-10 Batch 4:  Loss -  0.220192 Accuracy -  0.688\n",
      "Epoch 114, CIFAR-10 Batch 5:  Loss -  0.19257 Accuracy -  0.6816\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss -  0.296861 Accuracy -  0.6856\n",
      "Epoch 115, CIFAR-10 Batch 2:  Loss -  0.301717 Accuracy -  0.6814\n",
      "Epoch 115, CIFAR-10 Batch 3:  Loss -  0.207111 Accuracy -  0.6832\n",
      "Epoch 115, CIFAR-10 Batch 4:  Loss -  0.215452 Accuracy -  0.6872\n",
      "Epoch 115, CIFAR-10 Batch 5:  Loss -  0.210818 Accuracy -  0.69\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss -  0.302444 Accuracy -  0.6812\n",
      "Epoch 116, CIFAR-10 Batch 2:  Loss -  0.268906 Accuracy -  0.691\n",
      "Epoch 116, CIFAR-10 Batch 3:  Loss -  0.182565 Accuracy -  0.6834\n",
      "Epoch 116, CIFAR-10 Batch 4:  Loss -  0.216167 Accuracy -  0.6906\n",
      "Epoch 116, CIFAR-10 Batch 5:  Loss -  0.208136 Accuracy -  0.6866\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss -  0.251807 Accuracy -  0.6934\n",
      "Epoch 117, CIFAR-10 Batch 2:  Loss -  0.265846 Accuracy -  0.6882\n",
      "Epoch 117, CIFAR-10 Batch 3:  Loss -  0.189176 Accuracy -  0.6896\n",
      "Epoch 117, CIFAR-10 Batch 4:  Loss -  0.21684 Accuracy -  0.69\n",
      "Epoch 117, CIFAR-10 Batch 5:  Loss -  0.204666 Accuracy -  0.6814\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss -  0.31372 Accuracy -  0.677\n",
      "Epoch 118, CIFAR-10 Batch 2:  Loss -  0.299688 Accuracy -  0.688\n",
      "Epoch 118, CIFAR-10 Batch 3:  Loss -  0.216996 Accuracy -  0.6812\n",
      "Epoch 118, CIFAR-10 Batch 4:  Loss -  0.210039 Accuracy -  0.6866\n",
      "Epoch 118, CIFAR-10 Batch 5:  Loss -  0.191083 Accuracy -  0.6824\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss -  0.276655 Accuracy -  0.6882\n",
      "Epoch 119, CIFAR-10 Batch 2:  Loss -  0.278521 Accuracy -  0.694\n",
      "Epoch 119, CIFAR-10 Batch 3:  Loss -  0.163606 Accuracy -  0.6874\n",
      "Epoch 119, CIFAR-10 Batch 4:  Loss -  0.236031 Accuracy -  0.683\n",
      "Epoch 119, CIFAR-10 Batch 5:  Loss -  0.208383 Accuracy -  0.6918\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss -  0.271601 Accuracy -  0.6854\n",
      "Epoch 120, CIFAR-10 Batch 2:  Loss -  0.283536 Accuracy -  0.6884\n",
      "Epoch 120, CIFAR-10 Batch 3:  Loss -  0.176475 Accuracy -  0.6878\n",
      "Epoch 120, CIFAR-10 Batch 4:  Loss -  0.185992 Accuracy -  0.696\n",
      "Epoch 120, CIFAR-10 Batch 5:  Loss -  0.17906 Accuracy -  0.6882\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss -  0.291524 Accuracy -  0.6786\n",
      "Epoch 121, CIFAR-10 Batch 2:  Loss -  0.298502 Accuracy -  0.6778\n",
      "Epoch 121, CIFAR-10 Batch 3:  Loss -  0.163126 Accuracy -  0.6916\n",
      "Epoch 121, CIFAR-10 Batch 4:  Loss -  0.231114 Accuracy -  0.6832\n",
      "Epoch 121, CIFAR-10 Batch 5:  Loss -  0.205792 Accuracy -  0.6946\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss -  0.28798 Accuracy -  0.6944\n",
      "Epoch 122, CIFAR-10 Batch 2:  Loss -  0.307397 Accuracy -  0.6948\n",
      "Epoch 122, CIFAR-10 Batch 3:  Loss -  0.184359 Accuracy -  0.6944\n",
      "Epoch 122, CIFAR-10 Batch 4:  Loss -  0.23651 Accuracy -  0.6828\n",
      "Epoch 122, CIFAR-10 Batch 5:  Loss -  0.224911 Accuracy -  0.6872\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss -  0.260282 Accuracy -  0.6906\n",
      "Epoch 123, CIFAR-10 Batch 2:  Loss -  0.301042 Accuracy -  0.6926\n",
      "Epoch 123, CIFAR-10 Batch 3:  Loss -  0.180253 Accuracy -  0.692\n",
      "Epoch 123, CIFAR-10 Batch 4:  Loss -  0.220545 Accuracy -  0.69\n",
      "Epoch 123, CIFAR-10 Batch 5:  Loss -  0.194101 Accuracy -  0.6958\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss -  0.336728 Accuracy -  0.6874\n",
      "Epoch 124, CIFAR-10 Batch 2:  Loss -  0.28001 Accuracy -  0.6872\n",
      "Epoch 124, CIFAR-10 Batch 3:  Loss -  0.187874 Accuracy -  0.6882\n",
      "Epoch 124, CIFAR-10 Batch 4:  Loss -  0.218114 Accuracy -  0.6858\n",
      "Epoch 124, CIFAR-10 Batch 5:  Loss -  0.183172 Accuracy -  0.6844\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss -  0.270018 Accuracy -  0.6848\n",
      "Epoch 125, CIFAR-10 Batch 2:  Loss -  0.273276 Accuracy -  0.6948\n",
      "Epoch 125, CIFAR-10 Batch 3:  Loss -  0.175883 Accuracy -  0.69\n",
      "Epoch 125, CIFAR-10 Batch 4:  Loss -  0.190019 Accuracy -  0.6882\n",
      "Epoch 125, CIFAR-10 Batch 5:  Loss -  0.182888 Accuracy -  0.6872\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss -  0.205841 Accuracy -  0.6924\n",
      "Epoch 126, CIFAR-10 Batch 2:  Loss -  0.299042 Accuracy -  0.6972\n",
      "Epoch 126, CIFAR-10 Batch 3:  Loss -  0.15352 Accuracy -  0.6902\n",
      "Epoch 126, CIFAR-10 Batch 4:  Loss -  0.246249 Accuracy -  0.6846\n",
      "Epoch 126, CIFAR-10 Batch 5:  Loss -  0.196732 Accuracy -  0.687\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss -  0.234949 Accuracy -  0.693\n",
      "Epoch 127, CIFAR-10 Batch 2:  Loss -  0.28722 Accuracy -  0.6934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 127, CIFAR-10 Batch 3:  Loss -  0.160852 Accuracy -  0.6898\n",
      "Epoch 127, CIFAR-10 Batch 4:  Loss -  0.206765 Accuracy -  0.6928\n",
      "Epoch 127, CIFAR-10 Batch 5:  Loss -  0.19004 Accuracy -  0.6896\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss -  0.282864 Accuracy -  0.6786\n",
      "Epoch 128, CIFAR-10 Batch 2:  Loss -  0.278499 Accuracy -  0.6856\n",
      "Epoch 128, CIFAR-10 Batch 3:  Loss -  0.144049 Accuracy -  0.6852\n",
      "Epoch 128, CIFAR-10 Batch 4:  Loss -  0.23953 Accuracy -  0.6882\n",
      "Epoch 128, CIFAR-10 Batch 5:  Loss -  0.17622 Accuracy -  0.6892\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./image_classification\n",
      "Testing Accuracy: 0.6852254746835443\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+kulvc9vriAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1806b208>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. That's because there are many more techniques that can be applied to your model and we recemmond that once you are done with this project, you explore!\n",
    "\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
